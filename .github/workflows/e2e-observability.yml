name: E2E Observability Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'services/**'
      - 'tests/e2e/**'
      - 'config/observe/**'
      - 'docker-compose*.yml'
      - '.github/workflows/e2e-observability.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'services/**'
      - 'tests/e2e/**'
      - 'config/observe/**'
      - 'docker-compose*.yml'
  schedule:
    # Run tests daily at 2 AM UTC to catch environment drift
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_pattern:
        description: 'Pytest pattern to run (e.g., test_metrics or TestIntegration)'
        required: false
        default: ''
      log_level:
        description: 'Log level for tests'
        required: false
        default: 'INFO'
        type: choice
        options:
          - DEBUG
          - INFO
          - WARNING
          - ERROR

jobs:
  e2e-observability-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    strategy:
      matrix:
        python-version: ["3.11"]
        test-suite: ["metrics", "traces", "logs", "integration"]
      fail-fast: false
    
    env:
      DOCKER_BUILDKIT: 1
      COMPOSE_DOCKER_CLI_BUILD: 1
      BUILDX_NO_DEFAULT_LOAD: false
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Cache Docker layers
      uses: actions/cache@v4
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-e2e-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-buildx-e2e-
          ${{ runner.os }}-buildx-

    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('tests/e2e/requirements-e2e.txt', 'requirements-dev.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Free up disk space
      run: |
        docker system prune -af
        sudo rm -rf /usr/local/lib/android /usr/share/dotnet /opt/ghc
        df -h
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install local dependencies for test utilities
      run: |
        pip install --upgrade pip
        pip install docker-compose pytest
    
    - name: Validate Docker Compose configuration
      run: |
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml config --quiet
    
    - name: Build Docker images
      run: |
        # Build inference service image
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml build inference
        
        # Build test runner image
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml build test-runner
    
    - name: Start observability stack
      run: |
        # Start all services except test-runner
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml up -d \
          inference prometheus jaeger loki promtail grafana
    
    - name: Wait for services to be healthy
      timeout-minutes: 10
      run: |
        echo "Waiting for services to become healthy..."
        
        # Function to check service health
        check_health() {
          local service=$1
          local max_attempts=60
          local attempt=0
          
          while [ $attempt -lt $max_attempts ]; do
            if docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml ps $service | grep -q "healthy"; then
              echo "✅ $service is healthy"
              return 0
            fi
            
            echo "⏳ Waiting for $service to be healthy (attempt $((attempt+1))/$max_attempts)..."
            sleep 5
            attempt=$((attempt+1))
          done
          
          echo "❌ $service failed to become healthy"
          docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml logs $service
          return 1
        }
        
        # Check each service
        check_health "inference"
        check_health "prometheus" 
        check_health "jaeger"
        check_health "loki"
        check_health "grafana"
        
        echo "All services are healthy!"
    
    - name: Show service status
      run: |
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml ps
        echo "Docker containers:"
        docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
    
    - name: Verify service endpoints
      run: |
        echo "Checking service endpoints..."
        
        # Check inference service
        curl -f http://localhost:8080/health || (echo "Inference health check failed" && exit 1)
        echo "✅ Inference service is responding"
        
        # Check Prometheus
        curl -f http://localhost:9090/-/ready || (echo "Prometheus ready check failed" && exit 1)
        echo "✅ Prometheus is responding"
        
        # Check Jaeger
        curl -f http://localhost:16686/api/services || (echo "Jaeger API check failed" && exit 1)
        echo "✅ Jaeger is responding"
        
        # Check Loki
        curl -f http://localhost:3100/ready || (echo "Loki ready check failed" && exit 1)
        echo "✅ Loki is responding"
        
        # Check Grafana
        curl -f http://localhost:3000/api/health || (echo "Grafana health check failed" && exit 1)
        echo "✅ Grafana is responding"
    
    - name: Generate baseline observability data
      run: |
        echo "Generating baseline data..."
        # Make a few requests to generate initial metrics/traces/logs
        for i in {1..5}; do
          curl -X POST http://localhost:8080/predict \
            -H "Content-Type: application/json" \
            -d "{\"text\": \"Baseline test $i\", \"model\": \"primary\"}" || true
          sleep 1
        done
        
        # Wait for data to propagate
        sleep 10
        
        # Check that we have some metrics
        curl -s "http://localhost:9090/api/v1/query?query=up" | jq '.data.result | length' || true
    
    - name: Determine test pattern for matrix
      id: test-pattern
      run: |
        case "${{ matrix.test-suite }}" in
          "metrics")
            PATTERN="test_metrics or TestMetrics"
            ;;
          "traces")
            PATTERN="test_trac or TestTracing"
            ;;
          "logs")
            PATTERN="test_log or TestLogs"
            ;;
          "integration")
            PATTERN="TestObservabilityIntegration or TestDashboard"
            ;;
          *)
            PATTERN=""
            ;;
        esac
        
        # Override with manual pattern if provided
        if [ -n "${{ github.event.inputs.test_pattern }}" ]; then
          PATTERN="${{ github.event.inputs.test_pattern }}"
        fi
        
        echo "pattern=$PATTERN" >> $GITHUB_OUTPUT
        echo "Test pattern for ${{ matrix.test-suite }}: $PATTERN"

    - name: Run E2E observability tests
      env:
        TEST_INFERENCE_URL: http://localhost:8080
        TEST_PROMETHEUS_URL: http://localhost:9090
        TEST_JAEGER_URL: http://localhost:16686
        TEST_LOKI_URL: http://localhost:3100
        TEST_GRAFANA_URL: http://localhost:3000
        TEST_TIMEOUT: 300
        LOG_LEVEL: ${{ github.event.inputs.log_level || 'INFO' }}
        PYTHONPATH: ${{ github.workspace }}
        PYTEST_WORKERS: "1"  # Avoid parallel conflicts with Docker resources
      run: |
        TEST_PATTERN="${{ steps.test-pattern.outputs.pattern }}"
        PYTEST_ARGS="tests/e2e/ -v --tb=short --maxfail=3 --disable-warnings"
        
        if [ -n "$TEST_PATTERN" ]; then
          PYTEST_ARGS="$PYTEST_ARGS -k '$TEST_PATTERN'"
        fi
        
        # Add test result output
        PYTEST_ARGS="$PYTEST_ARGS --junitxml=test-results-${{ matrix.test-suite }}.xml"
        
        echo "Running ${{ matrix.test-suite }} tests with: pytest $PYTEST_ARGS"
        
        # Run tests using the test runner container
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml run \
          --rm \
          --name e2e-test-runner-${{ matrix.test-suite }} \
          -e PYTEST_CURRENT_TEST \
          test-runner pytest $PYTEST_ARGS

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.test-suite }}
        path: test-results-${{ matrix.test-suite }}.xml
        retention-days: 7
    
    - name: Collect test artifacts on failure
      if: failure()
      run: |
        echo "Collecting diagnostic information..."
        
        # Create artifacts directory
        mkdir -p test-artifacts
        
        # Collect service logs
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml logs --no-color > test-artifacts/docker-compose-logs.txt 2>&1 || true
        
        # Collect individual service logs
        for service in inference prometheus jaeger loki promtail grafana; do
          echo "Collecting logs for $service..."
          docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml logs --no-color $service > test-artifacts/$service-logs.txt 2>&1 || true
        done
        
        # Collect service status
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml ps > test-artifacts/service-status.txt 2>&1 || true
        docker ps -a > test-artifacts/docker-ps.txt 2>&1 || true
        
        # Collect Prometheus targets and config
        curl -s http://localhost:9090/api/v1/targets > test-artifacts/prometheus-targets.json 2>&1 || true
        curl -s http://localhost:9090/api/v1/status/config > test-artifacts/prometheus-config.json 2>&1 || true
        
        # Collect some sample metrics
        curl -s "http://localhost:9090/api/v1/query?query=up" > test-artifacts/prometheus-up-metrics.json 2>&1 || true
        
        # Collect Jaeger services
        curl -s http://localhost:16686/api/services > test-artifacts/jaeger-services.json 2>&1 || true
        
        # System resources
        df -h > test-artifacts/disk-usage.txt 2>&1 || true
        free -h > test-artifacts/memory-usage.txt 2>&1 || true
        
        echo "Artifacts collected in test-artifacts/"
    
    - name: Upload test artifacts
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-artifacts-${{ github.run_number }}
        path: test-artifacts/
        retention-days: 7
    
    - name: Generate test report summary
      if: always()
      run: |
        echo "# E2E Observability Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ job.status }}" = "success" ]; then
          echo "✅ **All tests passed!**" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Tests failed. Check the logs above and artifacts for details.**" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Service Status" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml ps >> $GITHUB_STEP_SUMMARY 2>&1 || true
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Python version:** ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Test pattern:** ${{ github.event.inputs.test_pattern || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Log level:** ${{ github.event.inputs.log_level || 'INFO' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
    
    - name: Cleanup
      if: always()
      run: |
        echo "Cleaning up..."
        
        # Stop and remove containers
        docker-compose -f docker-compose.yml -f tests/e2e/docker-compose.test.yml down -v --remove-orphans || true
        
        # Clean up test images and containers
        docker system prune -f || true
        
        # Show remaining disk usage
        df -h

  # Collect and summarize results from all test suites
  collect-results:
    name: Collect Test Results
    runs-on: ubuntu-latest
    needs: e2e-observability-tests
    if: always()
    
    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: test-results/
          merge-multiple: true

      - name: Install test result parser
        run: |
          pip install junitparser

      - name: Parse test results
        run: |
          python3 << 'EOF'
          import xml.etree.ElementTree as ET
          import glob
          import json
          
          results = {}
          total_tests = 0
          total_failures = 0
          total_errors = 0
          
          for xml_file in glob.glob('test-results/*.xml'):
              try:
                  tree = ET.parse(xml_file)
                  root = tree.getroot()
                  
                  suite_name = xml_file.split('/')[-1].replace('test-results-', '').replace('.xml', '')
                  tests = int(root.get('tests', 0))
                  failures = int(root.get('failures', 0))
                  errors = int(root.get('errors', 0))
                  time = float(root.get('time', 0))
                  
                  results[suite_name] = {
                      'tests': tests,
                      'failures': failures,
                      'errors': errors,
                      'time': time,
                      'success': failures == 0 and errors == 0
                  }
                  
                  total_tests += tests
                  total_failures += failures
                  total_errors += errors
                  
              except Exception as e:
                  print(f"Error parsing {xml_file}: {e}")
          
          # Generate summary
          print(f"TOTAL_TESTS={total_tests}")
          print(f"TOTAL_FAILURES={total_failures}")
          print(f"TOTAL_ERRORS={total_errors}")
          print(f"SUCCESS_RATE={((total_tests - total_failures - total_errors) / max(total_tests, 1)) * 100:.1f}")
          
          # Save detailed results
          with open('summary.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          EOF

      - name: Generate test summary
        run: |
          # Read summary data
          TOTAL_TESTS=$(python3 -c "import json; data=json.load(open('summary.json')); print(sum(r['tests'] for r in data.values()))")
          TOTAL_FAILURES=$(python3 -c "import json; data=json.load(open('summary.json')); print(sum(r['failures'] for r in data.values()))")
          TOTAL_ERRORS=$(python3 -c "import json; data=json.load(open('summary.json')); print(sum(r['errors'] for r in data.values()))")
          SUCCESS_RATE=$(python3 -c "import json; data=json.load(open('summary.json')); total=sum(r['tests'] for r in data.values()); passed=total-sum(r['failures'] for r in data.values())-sum(r['errors'] for r in data.values()); print(f'{(passed/max(total,1)*100):.1f}')")
          
          echo "# E2E Observability Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Failures**: $TOTAL_FAILURES" >> $GITHUB_STEP_SUMMARY
          echo "- **Errors**: $TOTAL_ERRORS" >> $GITHUB_STEP_SUMMARY
          echo "- **Success Rate**: $SUCCESS_RATE%" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "| Suite | Tests | Failures | Errors | Duration | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------|----------|--------|----------|--------|" >> $GITHUB_STEP_SUMMARY
          
          python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import json
          data = json.load(open('summary.json'))
          for suite, results in data.items():
              status = "✅" if results['success'] else "❌"
              print(f"| {suite} | {results['tests']} | {results['failures']} | {results['errors']} | {results['time']:.1f}s | {status} |")
          EOF

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-summary
          path: |
            summary.json
            test-results/
          retention-days: 14

  # Notify on failure
  notify-on-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [e2e-observability-tests, collect-results]
    if: failure() && (github.event_name == 'schedule' || github.event_name == 'push')
    
    steps:
      - name: Create failure notification
        run: |
          echo "🚨 **E2E Observability Tests Failed**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Event**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Time**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Please check the test artifacts and logs for details." >> $GITHUB_STEP_SUMMARY
          
          # TODO: Add actual notification logic here
          # - Slack webhook
          # - Email notification
          # - PagerDuty alert
          echo "Notification logic would trigger here for production setup"
